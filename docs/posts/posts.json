[
  {
    "path": "posts/2023-02-03-florida-fatalities-part-2/",
    "title": "Analysis of Fatal Traffic Crashes in Florida: Part 2",
    "description": "This post is a continuation of post \"Analysis of Fatal Traffic Crashes in Florida: Part 1.\" In this post, I will identify road intersections closest to the crash locations in Brevard County, Florida, as an example.",
    "author": [
      {
        "name": "Mohamed Badhrudeen",
        "url": "https://mohamedbadhrudeen.github.io/about"
      }
    ],
    "date": "2023-02-03",
    "categories": [],
    "contents": "\r\nThis post is a continuation of an earlier post I made. If you have\r\nnot read it, you can read that post here.\r\nAnalysis\r\nI am using the same data used in my earlier post. The first step is\r\nto import the necessary packages: sfnetworks, igraph,\r\ntidygraph, and sf. The sfnetworks package can\r\nconvert the sf object into a network object. It makes our analysis way\r\nmuch easier. The igraph and tidygraph packages are\r\nused to extract centrality measures of an intersection.\r\n\r\n\r\n\r\n\r\n\r\nFloridaCounties009 = st_read(\"./newfolder/tl_2020_12009_roads.shp\", quiet = TRUE)\r\n\r\n#Define the desired projection\r\nproj = \"+proj=utm +zone=17 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs +type=crs\"\r\n#Projecting the shapefile to the defined projection\r\nFloridaCounties009Proj = st_transform(FloridaCounties009, crs = proj)\r\n\r\n#Creating spatial object from the latitude and longitude \r\n#information in the FloridaCrashData\r\nCrashLocations = FloridaCrashData %>% \r\n  select(COUNTYNAME, LONGITUD, LATITUDE, TWAY_ID, TWAY_ID2, YEAR) %>% \r\n  filter(grepl(\"BREVARD\", COUNTYNAME, ignore.case = TRUE)) %>% \r\n  st_as_sf(., coords = c('LONGITUD', 'LATITUDE'), crs = 4269)\r\n\r\n\r\n\r\nI have selected the BREVARD county for the\r\nillustration purpose. Firstly, let’s import the shapefile of that\r\ncounty’s road networks. Followed by transforming the shapefile into the\r\ndesired projection. Finally, I extracted the latitude and longitude\r\ninformation and converted them into an sf object.\r\n\r\n\r\n# Create the igraph of road networks using sfnetworks package\r\ng <- as_sfnetwork(x = FloridaCounties009, \r\n                              directed = FALSE, \r\n                              length_as_weight = TRUE, \r\n                              edges_as_lines = TRUE)\r\ng <- g %>%\r\n  activate(\"nodes\") %>% \r\n  mutate(BetweennessCentrality = centrality_betweenness(\r\n    weights = weight, normalized = TRUE, \r\n    directed = FALSE)) %>%\r\n  mutate(DegreeCentrality = centrality_degree(normalized = TRUE) )\r\n\r\n\r\n\r\nHere, I use the sfnetwork package to create a network object\r\nfrom the shapefile I imported. The network object is stored in the\r\nvariable g. At this point we can calculate any\r\ncentrality measures of an intersection. As an example, I calculated\r\ncentrality_betweenness and centrality_degree. You can\r\ncalculate any desired network measures at this stage. Let’s look at the\r\ndistribution of Degree Centrality measure.\r\n\r\n\r\n\r\nIt seems that there are more dead ends (degree of 1 means dead ends).\r\nIt is a bit off to have this many dead ends. I think the sfnetworks is\r\nnot performing well. Let’s not dwell in this for now as we are not\r\ninterested in that. Now, we can extract teh location of each\r\nintersection using the following code. One thing to notice here is that\r\nwhen we are interested in nodes, we need to activate nodes first. If you\r\nare interested in edges, you need to activate edges first then perform\r\nthe analysis.\r\n\r\n\r\n#Extract the all intersection coordinates from the graph\r\nnode_coords <- g %>% activate(\"nodes\") %>% \r\n  st_coordinates() %>% as.data.frame %>% \r\n  st_as_sf(coords = c(\"X\",\"Y\"), crs = 4269) \r\n\r\n#Extract the location of crashes\r\ncrash_points <- CrashLocations %>% st_coordinates() %>% \r\n  as.data.frame %>% \r\n  st_as_sf(coords = c(\"X\",\"Y\"), crs = 4269)\r\n\r\n\r\n\r\nWe extracted all the points of intersections from the network object,\r\nand all points (location) of crash sites. Now, for a give crash site\r\nlocation, We can calculate the distance between that crash location and\r\nall intersection points. Then we can arrange them in ascending order and\r\npick the first value and the index of the node (i.e. intersection).\r\n\r\n\r\n#Identifying the closest intersections based on computing distance\r\nind_ <- c()\r\n\r\nfor(i in 1:length(crash_points$geometry)) {\r\n  d <- st_distance(crash_points$geometry[i], node_coords$geometry)\r\n  ind_ <- c(ind_, which(d == min(d))) }\r\n\r\n\r\n\r\nHere, I have extracted all the indices of intersections that are\r\nclosest to the crash locations.\r\n\r\n\r\n# Extract the intersection points\r\ncrash_points <- cbind(ind_, crash_points)  \r\nIntersectionsNearCrashPoints <- node_coords %>% slice(ind_) %>% \r\n  st_transform(., crs = proj)\r\n\r\n\r\n\r\nLet’s plot them and see. The green dots are crash location and red\r\nsquares are the intersection closest to a crash location.\r\n\r\n\r\n#Plotting the crash locations and intersections near crash locations\r\ntm_shape(FloridaCounties009Proj) + tm_lines() +\r\n  tm_shape(CrashLocations) + tm_dots(size = .1, col = 'green') +\r\n  tm_shape(IntersectionsNearCrashPoints) + tm_dots(shape = 0, \r\n                                                   size = .1, col = 'red')\r\n\r\n\r\n\r\n\r\nEt Voilà! That’s it for this post. Once we have the\r\nintersection information i.e. latitude and longitude, closest to the\r\ncrash site, we can find many road network information specific to that\r\ncrash location. This information can be integrated in our analyses to\r\nmake accurate prediction (or inferences).\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-03-florida-fatalities-part-2/florida-fatalities-part-2_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2023-08-08T18:03:55-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-06-traffic-fatalities-in-florida/",
    "title": "Analysis of Fatal Traffic Crashes in Florida: Part 1",
    "description": "In this post, I will look at the the fatal accidents over a period of five years in Florida at different counties to see how the crash rate varies over time. Moreover, I will use a statistical test to identify counties where there is an increase or decrese in crash rate. Finally, I will plot the locations of the crashes, and identfy the nearest road intersections for an example county. Finally, overlay the accidents and intersection locations and visualize it using **tmap** package.",
    "author": [
      {
        "name": "Mohamed Badhrudeen",
        "url": "https://mohamedbadhrudeen.github.io/about"
      }
    ],
    "date": "2023-01-06",
    "categories": [],
    "contents": "\r\nBackground\r\nTraffic safety is one of the important facets in transportation\r\nengineering. Many initiatives have been undertaken by various Department\r\nof Transportation agencies in the United States to make commuting safer.\r\nIn recent times, the potential of autonomous and connected vehicles in\r\nimproving the safety aspects of road transportation is being considered\r\nand initiatives are developed taking advantage of their benefits.\r\nMost of these initiatives are still being on the initial stages.\r\nTherefore, the jury is still out on how connected and autonomous\r\nvehicles will help reduce the traffic related accidents. On a abstract\r\nlevel there are two main types of research conducted in the traffic\r\nsafety research: a) finding the hotspots, i.e. the locations along the\r\nroadways where more accidents tend to happen, and b) assessing the\r\neffects of the combination of driver, vehicle, and environmental\r\ncharacteristics on the crash frequency. In general, an accident can be\r\nbroadly categorized into two kinds:\r\nfatal accidents (no deaths)\r\nnon-fatal accidents (vehicle damages and/or injuries)\r\nData\r\nIn this post, as a first step, I ventured to see if there are any\r\ntemporal differences in the occurrence of fatal accidents. The time time\r\nperiod chosen was five years, from 2016 to 2020. The shape files data\r\ncan be downloaded from here. As for the\r\nfatal crashes data for 5 years, I downloaded the data from here.\r\nThe data is for all the states in the United States. You need to subset\r\nthe data for the chosen state. In my case, I selected Florida. The\r\nreason is to identify the effects of topography, if any, by comparing it\r\nwith say Utah, which has different terrain than Florida. I will post the\r\nanalysis and results in my future posts.\r\nAnalysis\r\nThe first step is to see how fatal crashes distributed across\r\ndifferent counties in Florida. I am looking at the total fatal crashes\r\nrecorded within the selected time period of 5 years. At this point, we\r\njust want to know which counties have the highest recorded fatal crashes\r\nin Florida.\r\n\r\n\r\nfiles = list.files(\"./AccidentData\")\r\ndata = sapply(paste0(\"./AccidentData/\", files), read_csv)\r\n\r\n#Combining the data from all selected years  \r\nCombinedData = rbindlist(data, fill = TRUE)\r\n\r\n#Subset the data for Florida\r\nFloridaCrashData = CombinedData %>% filter(STATENAME == 'Florida')\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe plot above included all the crash count for all the selected\r\nyears. However, in order to statistically test the temporal difference,\r\nI decided to use a simple trend change identification test,\r\nMann-Kendall Test. The test can be applied to find if\r\nthe series either follows a monotonically increasing (decreasing) trend\r\nor not. In our case, we need to find if the series, i.e. the crash\r\ncounts over the five year period, either follows a monotonic trend or\r\nnot. If the series does not follow a monotonic trend then we can say\r\nthat there is no temporal difference in crash count. I must admit that\r\nthe series only contains 5 points, more points would give a better\r\nestimation. Therefore,\r\nNull hypothesis: No trend is observed\r\nAlternate hypothesis: A trend is observed (could be increasing or\r\ndecreasing)\r\n\r\n\r\ntrend_ = FloridaCrashData %>%\r\n    group_by(COUNTYNAME, YEAR) %>%\r\n    summarize(n = n()) %>% spread(YEAR, n)%>% replace(is.na(.), 0)\r\n\r\ntrend_ = trend_ %>% rowwise() %>% \r\n  mutate(P_Value = mk.test(c(`2016`, `2017`, `2018`, `2019`,`2020`))$p.value)\r\n\r\n\r\n\r\n\r\n\r\n\r\nLet’s look at the counties where there is a trend (either increasing\r\nor decreasing) in the series. We can just filer the P_Value\r\ncolumn with the condition that the p-value is less than 0.05. I used the\r\n95% confidence interval. So, if the county has a p-value less than 0.05,\r\nthen it means that there is a trend detected in the given series.\r\nTherefore, the crashes at those counties were either increased or\r\ndecreased over the years.\r\n\r\n\r\n\r\nFrom the above table, we can see that three counties in Florida the\r\nfatal crash frequency over the period of 5 years have seen a decreasing\r\ntrend. This result itself gives as the opportunity to focus on these\r\ncounties to see if they have implemented any traffic safety policies\r\nthat caused this decreasing trend. We can also compare these counties\r\nwith neighboring counties to see the differences in the selected\r\nvariables. For now, I will select the Brevard county\r\nfor plotting the locations of the crashes. The number right next the\r\ncounty name (9) is the county number. I have already downloaded the\r\nroads shapefiles for Brevard county. I will filter the\r\nFloridaCrashData by setting the COUNTY\r\nvariable equal to 9, and select the columns\r\nLatitude and Longitude.\r\n\r\n\r\nFloridaCounties009 = st_read(\"./newfolder/tl_2020_12009_roads.shp\", quiet = TRUE)\r\n\r\n#Define the desired projection\r\nproj = \"+proj=utm +zone=17 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs +type=crs\"\r\n#Projecting the shapefile to the defined projection\r\nFloridaCounties009Proj = st_transform(FloridaCounties009, crs = proj)\r\n\r\n#Creating spatial object from the latitude and longitude information in the FloridaCrashData\r\nCrashLocations = FloridaCrashData %>% \r\n  select(COUNTYNAME, LONGITUD, LATITUDE, TWAY_ID, TWAY_ID2, YEAR) %>% \r\n  filter(grepl(\"BREVARD\", COUNTYNAME, ignore.case = TRUE)) %>% \r\n  st_as_sf(., coords = c('LONGITUD', 'LATITUDE'), crs = 4269)\r\n\r\n\r\n\r\n\r\n\r\ntm_shape(FloridaCounties009Proj) + tm_lines() +\r\n  tm_shape(CrashLocations) + tm_dots(size = .04, col = 'green') + \r\n  tm_facets('YEAR')\r\n\r\n\r\n\r\n\r\nSo, that’s it for this post. In the next post, I will create a road\r\nnetworks graph of BREVARD county to extract the\r\nintersection information. Then, overlay the above map to identify the\r\nintersections closest to the crash locations. The idea is to use the\r\nintersection characteristics to study the effect of them in crash\r\nfrequency.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-06-traffic-fatalities-in-florida/traffic-fatalities-in-florida_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-08-08T18:03:54-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-12-ggplot-multiline-lables/",
    "title": "Creating Multiline Labels in R ggplot",
    "description": "In this post, we will create a function that takes a string and create a multiline output, which we will then use it for x and y lables of a ggplot.",
    "author": [
      {
        "name": "Mohamed Badhrudeen",
        "url": "https://mohamedbadhrudeen.github.io/about"
      }
    ],
    "date": "2022-04-12",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nBackground\r\nData\r\n\r\nBackground\r\nOne of the problems that I faced in the recent months was using y labels that are unusually long in ggplot. I tried abbreviate command in R, which did the job but it was not an optimal solution to my problem. So after spending a few days, I was able to write a function that takes a list of string as input along with maximum number of words in each line, the output is the multiline.\r\nData\r\nFor the illustrative purposes, I used the following data.\r\n\r\n\r\n#Downloaded and saved the data locally \r\ndata <- fread(\"Transport_Policy_instruments.csv\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nMy objective is to look at the category name and Sub schemes by Country. First, let’s see what are the unique category name in the data.\r\n\r\n\r\nunique(data[,`Category name`])\r\n\r\n\r\n[1] \"Fee/Charge\"                         \r\n[2] \"Tax\"                                \r\n[3] \"Environmentally motivated subsidies\"\r\n[4] \"Voluntary approaches\"               \r\n\r\nNow, let’s make a plot.\r\n\r\n\r\nggplot(data[, .N, by = .(Country, `Category name`)], \r\n       aes(x = Country, y = `Category name` , size = N)) + \r\n  geom_point() + \r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\r\n                         axis.text.y = element_text(angle = 45, hjust = 1)) +\r\n  scale_size(\"Count\")+ theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\nAs you can see, the y-label is too long making the plot look a little bit off. We can use the labels in scale_y_discrete option in ggplot. The function is below:\r\n\r\n\r\n#length.cutoff is the number of words ine ach line\r\nstring_multiline <- function(StringToChange, length.cutoff){\r\n  labels_ <- c() #to store the multilines\r\n  \r\n  for (j in StringToChange){ #list of strings to convert\r\n    \r\n    if (length(as.list(strsplit(j, \" \")[[1]])) <= length.cutoff){\r\n      labels_ <- c(labels_, j) #check to see if the string needs to be converted\r\n      next\r\n    }\r\n    \r\n    cf.split <- as.list(strsplit(j, \" \")[[1]])\r\n    cf.length <- length(cf.split)\r\n  \r\n    quo <- cf.length%/%length.cutoff\r\n    remainder <- cf.length%%length.cutoff\r\n  \r\n  \r\n    a <- \"\"\r\n    start <- c(1)\r\n  \r\n    for (i in 1:quo){\r\n      a <- paste( a, paste(cf.split[\r\n        start: (i * length.cutoff)], collapse = \" \"), collapse = \"\")\r\n      a <- paste(a, '\\n', sep =\"\")\r\n      if ( i == quo){\r\n        if ( remainder > 0){\r\n          a <- paste(a, paste(cf.split[\r\n            ((i * length.cutoff) + 1) :cf.length], collapse = \" \"), collapse =\" \")\r\n          a <- str_trim(a)\r\n        }\r\n      } else {\r\n        start <- (i * length.cutoff) + 1 \r\n      }\r\n    }\r\n    labels_ <- c(labels_, a)\r\n  }\r\n  \r\n  return(labels_)\r\n}\r\n\r\n\r\n\r\nNow, let’s see if the function to change the y lables works or not. First, let’s use length.cutoff as 2.\r\n\r\n\r\nggplot(data[, .N, by = .(Country, `Category name`)], \r\n       aes(x = Country, y = `Category name` , size = N)) + \r\n  geom_point() + \r\n  scale_y_discrete(labels = function(x) string_multiline(x, 2)) +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\r\n                         axis.text.y = element_text(angle = 45, hjust = .5)) +\r\n  scale_size(\"Count\")+ theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\nJust to make sure, let’s change the length.cutoff to 1.\r\n\r\n\r\n\r\nAfter all the time I spent on this, I later found out that there is a command in stringr package str_wrap that does the exact same thing. But the only difference is that instead of words counts on each line, the str_wrap command uses the width. See the code below:\r\n\r\n\r\nggplot(data[, .N, by = .(Country, `Category name`)], \r\n       aes(x = Country, y = `Category name` , size = N)) + \r\n  geom_point() + \r\n  scale_y_discrete(labels = function(x) str_wrap(x, 15)) +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\r\n                         axis.text.y = element_text(angle = 45, hjust = .5)) +\r\n  scale_size(\"Count\") + theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-12-ggplot-multiline-lables/ggplot-multiline-lables_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2023-08-08T18:03:53-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-01-freight-flow-visualization/",
    "title": "Visualization of US Domestic Freight Transport of Different Commodities Originating From Illinois",
    "description": "In this post, we will visualize the domestic freight transport flow of different commodities from Illinois to other US mainland states.",
    "author": [
      {
        "name": "Mohamed Badhrudeen",
        "url": "https://mohamedbadhrudeen.github.io/about"
      }
    ],
    "date": "2022-04-01",
    "categories": [],
    "contents": "\r\nRecently, I chanced upon an article written by Suzanne Greene, which was published in the MIT Climate portal. The article was brief and very informative. Reading that article sort of sent me down the proverbial rabbit hole where I read many articles on freight transportation and GHG emissions. I must admit there were a lot of information to digest. SO I decided to do a basic research on the topic.\r\nIn my view, it certainly looks easier for governments to implement policies to curb GHG emissions from passenger transportation than from freight transportation. For example, governments can implement policies to persuade people take public transit or take up active transport, but developing policies for freight transport is difficult. For freight transport relies heavily on fossil fuels and is needed to transport large amount of raw materials, finished products and so on. Furthermore, electrification, which is another potential solution to reduce GHG emissions, is relatively easier to adopt for smaller vehicles and for shorter travel distances than for the freight transport.\r\nTo learn more, I guess the starting point would be to see what the CO2 emissions have bee for different states over the last few years. The carbon emissions includes all sectors not just transportation. The data can be downloaded from here.\r\n\r\n\r\n#Downloaded and saved the data locally \r\nco2.emission <- fread('co2bystate.csv')\r\nco2.emission <- co2.emission[State != 'United States'] #Row that contains the total emissions\r\n\r\n#Used the melt function here to facilitate the plotting \r\nco2.melt <- melt(co2.emission, c(\"State\"), measure = patterns(\"20\"))\r\n\r\nggplot(co2.melt, aes(x = value, y = State, fill = State)) +\r\n  geom_density_ridges() + theme_ridges(font_size = 6,\r\n  grid = FALSE,\r\n  center_axis_labels = TRUE)+ theme(legend.position = \"none\") \r\n\r\n\r\n\r\n\r\nThe above figure shows that there are a few states that are leading others in CO2 emissions, notably, Texas, California, Illinois, Florida, Indiana. The rest of the post will focus on the flow of commodities from Illinois to other US mainland states, particularly domestic flows (no imports and exports). I used the data published by Bureau of Transportation Statistics (BTS). The data include estimates in weight and value. I am using the estimates (weights in thousand tons) for the year 2025. For more details on the data, please refer this pdf. Moreover, the visualization of flow of commodities is in part inspired by this blog.\r\nSo, let’s get started.\r\nI downloaded the data from the BTS website, and unziped and stored it in my local computer. The file is quite large, around 800 MB. From my experience, the R package data.table works really well in handling big data. I used a sample of the data to show you some calculation.\r\nSince, we are only concerned about the domestic transport, the trade_type is 1. Other information needed for the visualization are: 1. Name of the origin state 2. Name of the destination state\r\nThe problem is that dms_fips column values refers to the specific regions within states. Therefore, we need all the digits except the last one. For example, 131 is the value, which in its entirety corresponds to the following regions in Georgia: Atlanta, Athens, Clarke County and Sandy Springs. The first two digits i.e. 13 corresponds to the state. So, we need to extract that from the column.\r\nNow, it is time to plot some maps. So, we first need to download US states map. You can go on to the US Census Bureau website and download it. I downloaded one and stored it locally. To plot the commodities flow, we need the US mainland states map. We will only consider trade_type == 1 and orig_state == Illinois.\r\n\r\n\r\n\r\n\r\n\r\ndata_2 <- data_2[trade_type == 1 & orig_state == 'Illinois']\r\nstate_ <- st_read(\"shapefiles/cb_2018_us_state_500k.shp\")\r\n\r\n#Changing the projection to Albers\r\nstate_albers <- sf::st_transform(state_, 5070)\r\n#Calculate the Centroids, which we will use later\r\nstate_albers <- state_albers %>% mutate(lon = map_dbl(geometry, ~st_centroid(.x)[[1]]),\r\n                            lat = map_dbl(geometry, ~st_centroid(.x)[[2]]))\r\n#Selecting the mainland states\r\nstate_origin <- unique(data_2, by = \"dest_state\")[, dest_state]\r\nstate_origin_ <- setdiff(state_origin, c('Hawaii', 'Alaska'))\r\n\r\nstate_new <- state_albers[(state_$NAME) %in% state_origin_,]\r\n\r\n\r\n\r\nNow, we have the shapefile with all information we need, we will need to extract the inforamtion from the freight estimates. sctg2 is the column that contains the commodities type, and as mentioned earlier, trade_type is 1 for the domestic trips.\r\n\r\n\r\n#all domestic trips from Illinois to other states\r\nillinois_ <- data_2 %>% \r\n  group_by(orig_state, dest_state) %>% \r\n  summarise(avg_freight = mean(tons_2025),trips = n(), region = as.factor(mean(dest_region)))\r\n\r\n#Data for drawing the edges\r\n\r\nedges_ <- illinois_ %>%\r\n  inner_join(st_drop_geometry(state_new) %>% select(NAME, lon, lat), by = c('orig_state' = 'NAME')) %>%\r\n  rename(x.orig = lon, y.orig = lat) %>%\r\n  inner_join(st_drop_geometry(state_new) %>% select(NAME, lon, lat) , by = c('dest_state' = 'NAME')) %>%\r\n  rename(x.dest = lon, y.dest = lat) %>% filter(!dest_state %in% c('Hawaii', 'Alaska'))\r\n\r\n#Deleting teh Illinois to Illinois trips\r\n\r\nedges_1 <- filter(edges_, orig_state == \"Illinois\" & !dest_state %in% c('Hawaii', 'Alaska', \"Illinois\"))\r\n\r\n\r\n\r\nWe have all the data to plot the commodities flow from Illinois to other states. Let’s plot the map.\r\n\r\n\r\nmaptheme <- theme(panel.grid = element_blank()) +\r\n  theme(axis.text = element_blank()) +\r\n  theme(axis.ticks = element_blank()) +\r\n  theme(axis.title = element_blank()) +\r\n  theme(legend.position = \"bottom\") +\r\n  theme(panel.grid = element_blank()) +\r\n  theme(panel.background = element_rect(fill = \"#536243\")) +\r\n  theme(plot.margin = unit(c(0, 0, 0.5, 0), 'cm'))\r\n\r\nggplot(data = state_new) + geom_sf() + geom_point(aes(x = lon, y = lat)) +\r\n  geom_curve(data = edges_1, aes(x = x.orig, y = y.orig, xend = x.dest, yend = y.dest, \r\n                                 color  =  region,  size = trips), inherit.aes = FALSE, curvature = 0.33,\r\n             alpha = 0.5) + \r\n  scale_size_continuous(guide = \"none\", range = c(0.25, 2)) + \r\n  ggrepel::geom_text_repel(data= edges_1,aes(x=x.dest, y=y.dest, label=dest_state), \r\n                           color = \"darkblue\", fontface = \"bold\", size = 2, max.overlaps = 20) + \r\n  guides(colour = guide_legend(override.aes = list(alpha = 1))) + maptheme \r\n\r\n\r\n\r\n\r\nWe can further narrow down and see specific commodities flow. The type of commodities are denoted by the sctg2 column. For example, code 1 means Animals and Fish. We will select only Animals and Fish and see the flow now. We can simply add an extra condition sctg2 == 1 to data_2 just before group_by command.\r\n\r\n\r\n\r\nIt looks like the amount of animals and fish transported to Iowa, Michigan, Indiana, Wisconsin is larger than other neighboring states.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-01-freight-flow-visualization/freight-flow-visualization_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-08-08T18:03:53-04:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Extracting Information from 10Q filings",
    "description": "In this post, I explain how to extract financial information (statement of balance sheet) of **Apple Inc.** company from its third quarter 10Q filing for the year 2018.",
    "author": [
      {
        "name": "Mohamed Badhrudeen",
        "url": {}
      }
    ],
    "date": "2021-04-09",
    "categories": [],
    "contents": "\r\nThe information contains in this post is purely for educational purposes. I am not an R savant, so the code you will see below may not be efficient, but it does the job. Feel free to make it efficient.\r\nBackground\r\nOne of the ways I use to learn R is to pick a small side project, and break it down into multiple tasks. This post is part of a side project that I am working on, which is to extract balance sheet information of a company using their 10Q filings. I just want to start with one filing and extend it to multiple filings using a for loop. So lets start.\r\nAny company that is traded in US has to file its Quarterly financial data (unaudited), which is publicly available. You can also go for 10K filings, which is filed at the end each year. But I like to see the short term progress of a company more than its yearly progress. You can find the data at the SEC website. For the illustration purpose, lets use Apple Inc. Lets say we want to get Apple’s financial information for the year2018.\r\nData\r\nTo see the 10Q filings, go to SEC website, and type in the company’ name, and navigate to 10Q filings.\r\nOnce the appropriate 10Q filing is selected, you will see URL of that filing, copy it. As a side note there is an R package called “Edgar” that you can use to get the filings in HTML format and download it in your local drive.\r\n\r\n\r\nlibrary(stringr)\r\nlibrary(rvest)\r\nlibrary(XML)\r\nlibrary(gsubfn)\r\nlibrary(data.table)\r\n\r\nurl <- \"https://www.sec.gov/Archives/edgar/data/0000320193/000032019318000100/a10-qq320186302018.htm\"\r\n\r\n\r\n\r\nI have seen different companies using different terms in their filings, which is dependent on what kind of company it is. For example, Biotechnology companies use R&D expenses, whereas most Financial companies does not. So, make sure to use correct words if you are extracting a specific variable.\r\nFor balance sheet, I picked words “total assets” and “total liabilities”. You have to remember that strings are case sensitive. In some filings you might see “Total Assets”, in other filings “total assets”. I have not found any way to get around this problem. I think it is possible to convert the words in the html files into lower case and compare it to given string. Anyway, we will open each file, look for the words, and grab the first table, and extract all rows “tr” in the table.\r\n\r\n\r\n#get all lines in the document \r\nlines <- url %>%\r\n  read_html() %>%\r\n  html_nodes(xpath=\r\n  \"//table[contains(., 'Total assets') and contains(., 'Total liabilities')]\") %>%\r\n  html_nodes(xpath = 'tr') \r\n\r\n#Creating an empty table\r\ntable <- data.frame(variables = character(), values = double(), stringsAsFactors=FALSE)\r\nrow_ <- 1 #initialize row number \r\n\r\n\r\n\r\nThe above code look for all tables in the document and picks the table taht contains our words. Once the table is identified, it copies all rows into “lines” variable.\r\n\r\n\r\n#Look at each rows separately to extract the information\r\nfor(i in 1:length(lines)){\r\n  # get content of the line\r\n  linecontent <- lines[[i]]%>%\r\n    html_children()%>%\r\n    html_text()%>%\r\n    gsub(\"\\r\\n\",\"\",.)\r\n  \r\n  # attribute the content to free columns\r\n  if (grepl(\"[[:alpha:]]\", str_squish(linecontent[1]))  & nchar(linecontent[[1]]) < 30) { \r\n    # Check if the first cell contains any words and that word contains less than 30 letters \r\n    \r\n    val <- str_extract(\r\n      str_squish(linecontent[-1]), \r\n      \"[[:digit:]]+(,\\\\d+)*\") #If yes, then extract the line\r\n    \r\n    if (all(is.na(val), na.rm = TRUE)) {} \r\n    #Skipping lines that does not contain any numerical values \r\n    \r\n    else {\r\n      table[row_, ] <- #Add the extracted lines, and replace symbols like paranthesis. \r\n        c(str_squish(linecontent[1]), \r\n          gsubfn(\".\", list(\",\"=\"\", \"(\"= \"-\", \")\" = \"\"), \r\n                 linecontent[-1]\r\n                 [which(!is.na(\r\n                   str_extract(str_squish(linecontent[-1]), \r\n                                           \"[[:digit:]]+(,\\\\d+)*\")))[1]]))\r\n      row_ <- row_ + 1 }\r\n  }\r\n \r\n}\r\nbalance_sheets <-  as.data.frame(na.omit(table), \r\n                                 stringsAsFactors=FALSE) #store the table\r\nprint(balance_sheets)\r\n\r\n\r\n                       variables values\r\n1      Cash and cash equivalents  31971\r\n2       Accounts receivable, net  14104\r\n3                    Inventories   5936\r\n4   Vendor non-trade receivables  12263\r\n5           Other current assets  12488\r\n6           Total current assets 115761\r\n7       Other non-current assets  22546\r\n8                   Total assets 349197\r\n9               Accounts payable  38489\r\n10              Accrued expenses  25184\r\n11              Deferred revenue   7403\r\n12              Commercial paper  11974\r\n13     Total current liabilities  88548\r\n14 Deferred revenue, non-current   2878\r\n15                Long-term debt  97128\r\n16 Other non-current liabilities  45694\r\n17             Total liabilities 234248\r\n18             Retained earnings  79436\r\n19    Total shareholders’ equity 114949\r\n\r\nAnd, there you go.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-08-08T18:03:55-04:00",
    "input_file": {}
  }
]
