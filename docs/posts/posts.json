[
  {
    "path": "posts/2022-04-01-freight-flow-visualization/",
    "title": "Visualization of US Domestic Freight Transport of Different Commodities Originating From Illinois",
    "description": "In this post, we will visualize the domestic freight transport flow of different commodities from Illinois to other US mainland states.",
    "author": [
      {
        "name": "Mohamed Badhrudeen",
        "url": "https://mohamedbadhrudeen.github.io/about"
      }
    ],
    "date": "2022-04-01",
    "categories": [],
    "contents": "\r\nRecently, I chanced upon an article written by Suzanne Greene, which was published in the MIT Climate portal. The article was brief and very informative. Reading that article sort of sent me down the proverbial rabbit hole where I read many articles on freight transportation and GHG emissions. I must admit there were a lot of information to digest. SO I decided to do a basic research on the topic.\r\nIn my view, it certainly looks easier for governments to implement policies to curb GHG emissions from passenger transportation than from freight transportation. For example, governments can implement policies to persuade people take public transit or take up active transport, but developing policies for freight transport is difficult. For freight transport relies heavily on fossil fuels and is needed to transport large amount of raw materials, finished products and so on. Furthermore, electrification, which is another potential solution to reduce GHG emissions, is relatively easier to adopt for smaller vehicles and for shorter travel distances than for the freight transport.\r\nTo learn more, I guess the starting point would be to see what the CO2 emissions have bee for different states over the last few years. The carbon emissions includes all sectors not just transportation. The data can be downloaded from here.\r\n\r\n\r\n#Downloaded and saved the data locally \r\nco2.emission <- fread('co2bystate.csv')\r\nco2.emission <- co2.emission[State != 'United States'] #Row that contains the total emissions\r\n\r\n#Used the melt function here to facilitate the plotting \r\nco2.melt <- melt(co2.emission, c(\"State\"), measure = patterns(\"20\"))\r\n\r\nggplot(co2.melt, aes(x = value, y = State, fill = State)) +\r\n  geom_density_ridges() + theme_ridges(font_size = 6,\r\n  grid = FALSE,\r\n  center_axis_labels = TRUE)+ theme(legend.position = \"none\") \r\n\r\n\r\n\r\n\r\nThe above figure shows that there are a few states that are leading others in CO2 emissions, notably, Texas, California, Illinois, Florida, Indiana. The rest of the post will focus on the flow of commodities from Illinois to other US mainland states, particularly domestic flows (no imports and exports). I used the data published by Bureau of Transportation Statistics (BTS). The data include estimates in weight and value. I am using the estimates (weights in thousand tons) for the year 2025. For more details on the data, please refer this pdf. Moreover, the visualization of flow of commodities is in part inspired by this blog.\r\nSo, let’s get started.\r\nI downloaded the data from the BTS website, and unziped and stored it in my local computer. The file is quite large, around 800 MB. From my experience, the R package data.table works really well in handling big data. I used a sample of the data to show you some calculation.\r\nSince, we are only concerned about the domestic transport, the trade_type is 1. Other information needed for the visualization are: 1. Name of the origin state 2. Name of the destination state\r\nThe problem is that dms_fips column values refers to the specific regions within states. Therefore, we need all the digits except the last one. For example, 131 is the value, which in its entirety corresponds to the following regions in Georgia: Atlanta, Athens, Clarke County and Sandy Springs. The first two digits i.e. 13 corresponds to the state. So, we need to extract that from the column.\r\nNow, it is time to plot some maps. So, we first need to download US states map. You can go on to the US Census Bureau website and download it. I downloaded one and stored it locally. To plot the commodities flow, we need the US mainland states map. We will only consider trade_type == 1 and orig_state == Illinois.\r\n\r\n\r\n\r\n\r\n\r\ndata_2 <- data_2[trade_type == 1 & orig_state == 'Illinois']\r\nstate_ <- st_read(\"shapefiles/cb_2018_us_state_500k.shp\")\r\n\r\n#Changing the projection to Albers\r\nstate_albers <- sf::st_transform(state_, 5070)\r\n#Calculate the Centroids, which we will use later\r\nstate_albers <- state_albers %>% mutate(lon = map_dbl(geometry, ~st_centroid(.x)[[1]]),\r\n                            lat = map_dbl(geometry, ~st_centroid(.x)[[2]]))\r\n#Selecting the mainland states\r\nstate_origin <- unique(data_2, by = \"dest_state\")[, dest_state]\r\nstate_origin_ <- setdiff(state_origin, c('Hawaii', 'Alaska'))\r\n\r\nstate_new <- state_albers[(state_$NAME) %in% state_origin_,]\r\n\r\n\r\n\r\nNow, we have the shapefile with all information we need, we will need to extract the inforamtion from the freight estimates. sctg2 is the column that contains the commodities type, and as mentioned earlier, trade_type is 1 for the domestic trips.\r\n\r\n\r\n#all domestic trips from Illinois to other states\r\nillinois_ <- data_2 %>% \r\n  group_by(orig_state, dest_state) %>% \r\n  summarise(avg_freight = mean(tons_2025),trips = n(), region = as.factor(mean(dest_region)))\r\n\r\n#Data for drawing the edges\r\n\r\nedges_ <- illinois_ %>%\r\n  inner_join(st_drop_geometry(state_new) %>% select(NAME, lon, lat), by = c('orig_state' = 'NAME')) %>%\r\n  rename(x.orig = lon, y.orig = lat) %>%\r\n  inner_join(st_drop_geometry(state_new) %>% select(NAME, lon, lat) , by = c('dest_state' = 'NAME')) %>%\r\n  rename(x.dest = lon, y.dest = lat) %>% filter(!dest_state %in% c('Hawaii', 'Alaska'))\r\n\r\n#Deleting teh Illinois to Illinois trips\r\n\r\nedges_1 <- filter(edges_, orig_state == \"Illinois\" & !dest_state %in% c('Hawaii', 'Alaska', \"Illinois\"))\r\n\r\n\r\n\r\nWe have all the data to plot the commodities flow from Illinois to other states. Let’s plot the map.\r\n\r\n\r\nmaptheme <- theme(panel.grid = element_blank()) +\r\n  theme(axis.text = element_blank()) +\r\n  theme(axis.ticks = element_blank()) +\r\n  theme(axis.title = element_blank()) +\r\n  theme(legend.position = \"bottom\") +\r\n  theme(panel.grid = element_blank()) +\r\n  theme(panel.background = element_rect(fill = \"#536243\")) +\r\n  theme(plot.margin = unit(c(0, 0, 0.5, 0), 'cm'))\r\n\r\nggplot(data = state_new) + geom_sf() + geom_point(aes(x = lon, y = lat)) +\r\n  geom_curve(data = edges_1, aes(x = x.orig, y = y.orig, xend = x.dest, yend = y.dest, \r\n                                 color  =  region,  size = trips), inherit.aes = FALSE, curvature = 0.33,\r\n             alpha = 0.5) + \r\n  scale_size_continuous(guide = \"none\", range = c(0.25, 2)) + \r\n  ggrepel::geom_text_repel(data= edges_1,aes(x=x.dest, y=y.dest, label=dest_state), \r\n                           color = \"darkblue\", fontface = \"bold\", size = 2, max.overlaps = 20) + \r\n  guides(colour = guide_legend(override.aes = list(alpha = 1))) + maptheme \r\n\r\n\r\n\r\n\r\nWe can further narrow down and see specific commodities flow. The type of commodities are denoted by the sctg2 column. For example, code 1 means Animals and Fish. We will select only Animals and Fish and see the flow now. We can simply add an extra condition sctg2 == 1 to data_2 just before group_by command.\r\n\r\n\r\n\r\nIt looks like the amount of animals and fish transported to Iowa, Michigan, Indiana, Wisconsin is larger than other neighboring states.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-01-freight-flow-visualization/freight-flow-visualization_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-04-03T05:15:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Extracting Information from 10Q filings",
    "description": "In this post, I explain how to extract financial information (statement of balance sheet) of **Apple Inc.** company from its third quarter 10Q filing for the year 2018.",
    "author": [
      {
        "name": "Mohamed Badhrudeen",
        "url": {}
      }
    ],
    "date": "2021-04-09",
    "categories": [],
    "contents": "\r\nThe information contains in this post is purely for educational purposes. I am not an R savant, so the code you will see below may not be efficient, but it does the job. Feel free to make it efficient.\r\nBackground\r\nOne of the ways I use to learn R is to pick a small side project, and break it down into multiple tasks. This post is part of a side project that I am working on, which is to extract balance sheet information of a company using their 10Q filings. I just want to start with one filing and extend it to multiple filings using a for loop. So lets start.\r\nAny company that is traded in US has to file its Quarterly financial data (unaudited), which is publicly available. You can also go for 10K filings, which is filed at the end each year. But I like to see the short term progress of a company more than its yearly progress. You can find the data at the SEC website. For the illustration purpose, lets use Apple Inc. Lets say we want to get Apple’s financial information for the year2018.\r\nData\r\nTo see the 10Q filings, go to SEC website, and type in the company’ name, and navigate to 10Q filings.\r\nOnce the appropriate 10Q filing is selected, you will see URL of that filing, copy it. As a side note there is an R package called “Edgar” that you can use to get the filings in HTML format and download it in your local drive.\r\n\r\n\r\nlibrary(stringr)\r\nlibrary(rvest)\r\nlibrary(XML)\r\nlibrary(gsubfn)\r\nlibrary(data.table)\r\n\r\nurl <- \"https://www.sec.gov/Archives/edgar/data/0000320193/000032019318000100/a10-qq320186302018.htm\"\r\n\r\n\r\n\r\nI have seen different companies using different terms in their filings, which is dependent on what kind of company it is. For example, Biotechnology companies use R&D expenses, whereas most Financial companies does not. So, make sure to use correct words if you are extracting a specific variable.\r\nFor balance sheet, I picked words “total assets” and “total liabilities”. You have to remember that strings are case sensitive. In some filings you might see “Total Assets”, in other filings “total assets”. I have not found any way to get around this problem. I think it is possible to convert the words in the html files into lower case and compare it to given string. Anyway, we will open each file, look for the words, and grab the first table, and extract all rows “tr” in the table.\r\n\r\n\r\n#get all lines in the document \r\nlines <- url %>%\r\n  read_html() %>%\r\n  html_nodes(xpath=\r\n  \"//table[contains(., 'Total assets') and contains(., 'Total liabilities')]\") %>%\r\n  html_nodes(xpath = 'tr') \r\n\r\n#Creating an empty table\r\ntable <- data.frame(variables = character(), values = double(), stringsAsFactors=FALSE)\r\nrow_ <- 1 #initialize row number \r\n\r\n\r\n\r\nThe above code look for all tables in the document and picks the table taht contains our words. Once the table is identified, it copies all rows into “lines” variable.\r\n\r\n\r\n#Look at each rows separately to extract the information\r\nfor(i in 1:length(lines)){\r\n  # get content of the line\r\n  linecontent <- lines[[i]]%>%\r\n    html_children()%>%\r\n    html_text()%>%\r\n    gsub(\"\\r\\n\",\"\",.)\r\n  \r\n  # attribute the content to free columns\r\n  if (grepl(\"[[:alpha:]]\", str_squish(linecontent[1]))  & nchar(linecontent[[1]]) < 30) { \r\n    # Check if the first cell contains any words and that word contains less than 30 letters \r\n    \r\n    val <- str_extract(\r\n      str_squish(linecontent[-1]), \r\n      \"[[:digit:]]+(,\\\\d+)*\") #If yes, then extract the line\r\n    \r\n    if (all(is.na(val), na.rm = TRUE)) {} \r\n    #Skipping lines that does not contain any numerical values \r\n    \r\n    else {\r\n      table[row_, ] <- #Add the extracted lines, and replace symbols like paranthesis. \r\n        c(str_squish(linecontent[1]), \r\n          gsubfn(\".\", list(\",\"=\"\", \"(\"= \"-\", \")\" = \"\"), \r\n                 linecontent[-1]\r\n                 [which(!is.na(\r\n                   str_extract(str_squish(linecontent[-1]), \r\n                                           \"[[:digit:]]+(,\\\\d+)*\")))[1]]))\r\n      row_ <- row_ + 1 }\r\n  }\r\n \r\n}\r\nbalance_sheets <-  as.data.frame(na.omit(table), \r\n                                 stringsAsFactors=FALSE) #store the table\r\nprint(balance_sheets)\r\n\r\n\r\n                       variables values\r\n1      Cash and cash equivalents  31971\r\n2       Accounts receivable, net  14104\r\n3                    Inventories   5936\r\n4   Vendor non-trade receivables  12263\r\n5           Other current assets  12488\r\n6           Total current assets 115761\r\n7       Other non-current assets  22546\r\n8                   Total assets 349197\r\n9               Accounts payable  38489\r\n10              Accrued expenses  25184\r\n11              Deferred revenue   7403\r\n12              Commercial paper  11974\r\n13     Total current liabilities  88548\r\n14 Deferred revenue, non-current   2878\r\n15                Long-term debt  97128\r\n16 Other non-current liabilities  45694\r\n17             Total liabilities 234248\r\n18             Retained earnings  79436\r\n19    Total shareholders’ equity 114949\r\n\r\nAnd, there you go.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-01T19:36:04-05:00",
    "input_file": {}
  }
]
